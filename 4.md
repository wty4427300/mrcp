# 编写plugin的流程

tts语音合成

asr语音识别

在上一篇中我们提到过unimrcp提供了asr和tts的plugin的demo示例，基于demo可以让我们的开发更加快捷。一下两个过程介绍都是基于demo的。

## asr实现流程

1.SIP交互用于协商ASR过程中双方进行MRCP消息交互和媒体交互使用的端口和媒体格式等信息。  这一块一般无需改动，如果存在格式限制，可在配置文件 conf/unimrcpserver.xml 中配置 settings -> rtp-settings -> codecs ,如：
```
<codecs own-preference="false">PCMU PCMA L16/96/8000 PCMU/97/16000 PCMA/98/16000 L16/99/16000</codecs>
```
这里pcmu和pcma是两种pcm（一种音频文件格式）的无损压缩方式。

2.mrcpserver收到了recognize消息接受语音识别请求，进入asr流程。demo_recog_channel_recognize方法封装了recognize消息的处理。进入这个函数，表示收到一个语音识别请求，此请求处理成功后，会进入语音回调流程。

3.进入语音回调流程后，rtp流的交互也会开始。unimrcp的demo中语音回调函数是
```
static apt_bool_t demo_recog_stream_write(mpf_audio_stream_t *stream, const mpf_frame_t *frame)
```
4.检查语音的有效性，进行状态转换。如果要假如语音降噪处理vad应该这个环节后面加入，因为vad是一个cpu占用率比较高的环节，我们应该尽量检测出是否有空白音，去除无效语音长度，来降低vad环节的cpu消耗。
```
MPF_DECLARE(mpf_detector_event_e) mpf_activity_detector_process(mpf_activity_detector_t *detector, const mpf_frame_t *frame)
```
5.状态转换成功后返回发送asr识别响应，调用如下方法。

static apt_bool_t demo_recog_recognition_complete(demo_recog_channel_t *recog_channel, mrcp_recog_completion_cause_e cause)

demo中返回的是写死的固定识别

6.所以当我们基于这个demo实现asr的时候应该注意两点3.和5.。即传入语音调用语音识别服务，并修改5.写死的结果，返回真实的调用结果。

## tts实现

1.mrcpserver收到了speak消息接受语音合成请求，进入tts流程。
```
static apt_bool_t demo_synth_channel_speak(mrcp_engine_channel_t *channel, mrcp_message_t *request, mrcp_message_t *response)
```
进入这个函数，表示收到一个语音识别请求，此请求处理成功后，会进入语音回调流程。 

2.回调函数将语音传给缓存
```
static apt_bool_t demo_synth_stream_read(mpf_audio_stream_t *stream, mpf_frame_t *frame)
```
3.demo中是将处理后的语音读取一帧并填充到frame里，然后转化为rtp流发出去。

4.语音读取完之后发送完成消息tts流程结束。
```
/* send asynch event */
mrcp_engine_channel_message_send(synth_channel->channel,message);
```
5.所以基于demo开发tts要注意在1.取出文本，交给tts,以及将语音按顺序填充到buffer，发出去。由于合成需要时间所以可以先填充空白音。其次在合成语音转rtp发出去这里也可以不按照demo中的这种方式去写，无论怎么实现保证录音顺序就好。

ps:多看官方手册（上篇末尾）。本篇并没有具体的调用某个服务提供商的tts和asr服务，大家可以去尝试对接阿里，讯飞，中科院等等提供的相应服务。本系列基本完成了，如果还要写的话可能会写基于某个服务提供商实现完整demo的过程。



